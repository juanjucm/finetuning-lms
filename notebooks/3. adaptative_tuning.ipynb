{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "_GqY8kyCGdWL"
   },
   "source": [
    "# Adaptative tuning ü§ñ‚öôÔ∏è\n",
    "\n",
    "In this third notebook, we will perform MLM fine-tuning over a pre-trained BERT model in order to adapt it to a target domain. This will generate a BERT encoder able to better capture our dataset's semantics, hence leading to improved results on downstream tasks.\n",
    "\n",
    "Adaptative fine-tuning is performed using the same kins of unsupervised objectives that the ones performed during a from-scratch LM training. This is:\n",
    "* Masked Language Modeling: A random n% of the input tokens are masked and the model is asked to fill in the gaps.\n",
    "* Next Sentence Prediction: The model is asked to predict the whole next sentence of a text.\n",
    "\n",
    "\n",
    "<figure style='text-align:center';>\n",
    "  <img src=\"../data/images/AFT.png\">\n",
    "  \n",
    "  <figcaption>\n",
    "  Adaptative fine-tuning schema \n",
    "  </figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "**Due to the nature of our dataset, we will just perform MLM.**\n",
    "\n",
    "**Also keep in mind that there are different ways to do this. We will use a simple approach as demonstration, but more complex procedures are usually done (we will mention some of them).**\n",
    "\n",
    "Important points:\n",
    "* Dataset: [medical_questions_pairs](https://huggingface.co/datasets/medical_questions_pairs)\n",
    "* Model: [bert-base-cased](https://huggingface.co/bert-base-cased)\n",
    "* We will define auxiliar functions in auxiliar.py file\n",
    "* We will be logging the results in Weight&Biases.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "r1KTC0RxGdWR"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import config\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "   device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q1FgMRwhGdWU",
    "outputId": "4177f4a9-9d69-403a-d8c4-2bb1ff5e74a4"
   },
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "x0ibsd7DGdWV"
   },
   "source": [
    "## 1. Data preparation\n",
    "\n",
    "The data prep in this case will require a little bit more work.\n",
    "\n",
    "We will have to mask a random % of input tokens to create our training loop input.\n",
    "\n",
    "**Important: We will just use the training partition, since we don't want the model to see any of our test set. We keep that for the downstream task evaluation.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Dmo09TKBGdWW"
   },
   "source": [
    "### 1.1. Import and set creation\n",
    "\n",
    "Import data, create partitions and select train set. In order to better work with the data, let's export this to a pandas dataframe.\n",
    "\n",
    "**We have to replicate the process we have followed in previous notebooks, so the partitions are the same.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232,
     "referenced_widgets": [
      "165ff5abeb6a4d7ca88560ea4ba696f2",
      "43210f15b572459b85e7fa4a015eedb7",
      "b5ad8450d2304422bb4d496f191d0250",
      "46ab4bba8b2e4d22b272008b8047ae4e",
      "20336c9d7cd5435a9df3f1bfbc9be41a",
      "5a52dd56b169483dbf2bc71d3d438a99",
      "41c2d1cbfb5e4ce8873d2f61ee5ff24e",
      "e844e465435440f5bbe74a9e9ba879b1",
      "ba56c79bb02d4afb96eaa273e0c9845b",
      "0c1aeddfade14ef988634d59da85b47a",
      "f5f2139b4480446bb3d3717768de625f",
      "02f815eb70a04b7bbd494642dab27d01",
      "43633f9fb0b14dc08d6619661f401255",
      "e744cf290d474d69ae227e2f18eeffdb",
      "6f373853d68746eb82d0298b52ef31c0",
      "d5ff324a50674f778b7fe6d6b901ef27",
      "7b6bd5eeb98947e99eccfef14a6ab4fe",
      "e2edc1fce2b44f6f9cd05c06dbc778c3",
      "b6f12f53af2e43f2ad2f55b15d0b3bf0",
      "0935976d25e744d9a6fcd93a75dd2488",
      "1b05ba7494c44b4aa4b99257a922aac7",
      "6b1aa9e61bde4e018bfd7c280e64adf0",
      "61332facaed8462a88cd8b99409b9ee3",
      "3536481aba3f44e3ac0982dbb13423c6",
      "48637f5c306b4f0397a18b51b4d52ff0",
      "0ceecd5a42894a2eba34f4fc3c85bff5",
      "5e5b30c6b172421b95f05b8c49638845",
      "823d319731b34e49a3700dd4928c8c5b",
      "43b495c01e7c4985a75fbf077addeb08",
      "a4e27d7cd03f48f79cbba1b29f0df0b2",
      "615212bc939a4c398c8cc4565e766202",
      "d7db1129767b4cfea30f995dfac231c7",
      "7e2f03f3e0a343689d7173f68add0a2f",
      "e8f534232c8547b785182e2b98085ecc",
      "063663f3ee3844a990aef821ac5fa512",
      "e682cfd084f44cb48a054c29d45b9f89",
      "2f91d2da601840889f18e3bfac40c792",
      "cd2bf72c9937490ba8dceb1e5d115cba",
      "5553f32545ce4f19a04c750595b91c24",
      "fecb18d3cc184a46b7fd2caa5e3fe39b",
      "1e3ea96466d14af794ed63ac835fc9f7",
      "6f2fd19ea59d416391becf37d9b7af15",
      "7fa46fe0b58a48b38c5f128ce519c788",
      "4051216b52fb48519b1e9a2d762c5f7f",
      "72a88e2d34c84d3cb885a3fd21e8252d",
      "0eeee83dea8c40a18fc732cd46b5677e",
      "772f194696ec40cdad7a6690accf1f77",
      "ed749f31a6434382a23e83416086075a",
      "c929006bf77c40c6b25dec401140396e",
      "dff1a8fc5ea7407e8169297b465a3c73",
      "765e879874b84a85a11003e3c00848fa",
      "bd7c6ca3583642c3bba9dc45dd163e13",
      "a54dcd01e1314fdba74292528856f731",
      "4867b2223fc64bc68494acc02d0b9d08",
      "900f0796033f4b5aab759b73698aca56",
      "650328686d844480a972fc23e2cbc1c3",
      "ff54adcca3284e47977cc7ac7107776d",
      "7dec3ec25bd34335b1ee88bf77e5598b",
      "310a416818124278b3030abfb7812fd8",
      "78d0dec9d9a3418fb6bf69faca9350ba",
      "095d7fa53f36491f818a512cea5f5630",
      "051f4b8b55124ba09017ba7d68000e44",
      "f52f4c09baaa4f8392361e35c29ea94c",
      "4878e251a8a748f98694ead2154a1511",
      "02001706693a4617a7e643acf21d744f",
      "b35257ea02ba4923be366512a0cf8b3b"
     ]
    },
    "id": "WhAvqETXGdWW",
    "outputId": "82bd9edc-446a-4058-f29d-4bf118984fa2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset medical_questions_pairs (C:/Users/Juanju/.cache/huggingface/datasets/medical_questions_pairs/default/0.0.0/db30a35b934dceb7abed5ef6b73a432bb59682d00e26f9a1acd960635333bc80)\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 91.39it/s]\n",
      "Loading cached split indices for dataset at C:\\Users\\Juanju\\.cache\\huggingface\\datasets\\medical_questions_pairs\\default\\0.0.0\\db30a35b934dceb7abed5ef6b73a432bb59682d00e26f9a1acd960635333bc80\\cache-3a6913e31ee3f147.arrow and C:\\Users\\Juanju\\.cache\\huggingface\\datasets\\medical_questions_pairs\\default\\0.0.0\\db30a35b934dceb7abed5ef6b73a432bb59682d00e26f9a1acd960635333bc80\\cache-55366722f45172c0.arrow\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Download and extract data\n",
    "data = load_dataset(\"medical_questions_pairs\")\n",
    "data = data['train']\n",
    "\n",
    "# Split it\n",
    "dataset = data.train_test_split(test_size=0.07, seed=config.SEED)\n",
    "\n",
    "# Just keep the train partition\n",
    "dataset = dataset['train']\n",
    "\n",
    "# Export to pandas\n",
    "df = dataset.to_pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Dmo09TKBGdWW"
   },
   "source": [
    "### 1.2. Dataset modification\n",
    "\n",
    "Next step is to gather proper data for our training. We want to have a big list of sentences.\n",
    "\n",
    "Let's take a look at how our original dataset is composed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dr_id': [1, 1, 1],\n",
       " 'question_1': ['After how many hour from drinking an antibiotic can I drink alcohol?',\n",
       "  'After how many hour from drinking an antibiotic can I drink alcohol?',\n",
       "  'Am I over weight (192.9) for my age (39)?'],\n",
       " 'question_2': ['I have a party tonight and I took my last dose of Azithromycin this morning. Can I have a few drinks?',\n",
       "  'I vomited this morning and I am not sure if it is the side effect of my antibiotic or the alcohol I took last night...',\n",
       "  'I am a 39 y/o male currently weighing about 193 lbs. Do you think I am overweight?'],\n",
       " 'label': [1, 0, 1]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the field *question_1* contains repeated segments, since for each *question_1* there are two rephrasings in *question_2*.\n",
    "\n",
    "* For our usecase, we don't want duplicated sentences in the training set, but we DO want to also consider *question_2* field for training our MLM.\n",
    "* We will join all texts into a single list and remove duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join texts\n",
    "texts = df['question_1'].to_list() + df['question_2'].to_list()\n",
    "\n",
    "# Remove duplicates\n",
    "texts = list(set(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4351"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Dmo09TKBGdWW"
   },
   "source": [
    "### 1.3. Tokenization and encoding\n",
    "\n",
    "Last step of our preprocessing consists of tokenizing our texts and create the encodings.\n",
    "\n",
    "* Tokenize texts and create input_ids.\n",
    "* Insert [MASK] tokens randomly in our input_ids.\n",
    "* Create labels as a copy of our input_ids.\n",
    "* Build a dataset.\n",
    "* KUDOS to [James Briggs](https://www.youtube.com/watch?v=R6hcxMMOrPE) for the quick implementation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import torch\n",
    "\n",
    "def create_mlm_inputs(texts, tokenizer, percentage=0.15) -> Dict:\n",
    "    inputs = tokenizer(texts, return_tensors='pt', max_length=512, truncation=True, padding='max_length')\n",
    "\n",
    "    # Create labels as a clone of the input_ids.\n",
    "    inputs['labels'] = inputs.input_ids.detach().clone()\n",
    "\n",
    "    # Create mask filter\n",
    "    # We don't want to mask special tokens:\n",
    "    # 101 -> [CLS]\n",
    "    # 0 -> [PAD]\n",
    "    # 102 -> [SEP]\n",
    "    rand = torch.rand(inputs.input_ids.shape)\n",
    "    mask_filt = (rand < percentage) * (inputs.input_ids != 101) * (inputs.input_ids != 102) * (inputs.input_ids != 0)\n",
    "\n",
    "    # Mask tokens!\n",
    "    # For each sample, get mask_filt row and mask tokens at index.\n",
    "    for i in range(mask_filt.shape[0]):\n",
    "        mask_idxs = torch.flatten(mask_filt[i].nonzero()).tolist()\n",
    "        inputs.input_ids[i, mask_idxs] = 103\n",
    "    \n",
    "    return inputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = create_mlm_inputs(texts, tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##### **NOTE**\n",
    "\n",
    "Another option is tu make use of transformer's datacollator functionallity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, there are other kind of techniques for preparing input data in a wiser maner.\n",
    "* Joining all texts together and then split them in chunks (so we have less risk to truncation in larger datasets).\n",
    "* Apply word masking: instead of mask single tokens, we can mask subsequent tokens (words).\n",
    "\n",
    "Check [HuggingFace's tutorial](https://huggingface.co/course/chapter7/3?fw=tf#preprocessing-the-data) for a full guide "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for faster use, back again to a HF dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_data = Dataset.from_dict(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 4351\n",
       "})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "x0ibsd7DGdWV"
   },
   "source": [
    "## 2. Training\n",
    "\n",
    "Okay, we are ready to go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(config.checkpoint)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "7yI93JWlGdWg"
   },
   "source": [
    "### 2.1. Init WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "id": "2IX9tbfUGdWh",
    "outputId": "5b99fad8-99b4-49ff-e203-dd524632b922"
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "id": "O0uQPh8cGdWh",
    "outputId": "5dbf6e76-8966-4bdd-e745-1a47f11eb9e0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjjceamoran\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20221220_163325-wy3ea6og</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/jjceamoran/fine-tuning-mlms/runs/wy3ea6og\" target=\"_blank\">behavioural_training</a></strong> to <a href=\"https://wandb.ai/jjceamoran/fine-tuning-mlms\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_name = 'adaptative_training'\n",
    "notes = \"This experiment consists on performing MLM finetuning over a pre-trained bert with our dataset.\"\n",
    "run = wandb.init(project='fine-tuning-mlms',\n",
    "           name=run_name,\n",
    "           notes=notes,\n",
    "           job_type='train')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "7yI93JWlGdWg"
   },
   "source": [
    "### 2.2. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "pyLIBCZ2GdWh"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m Trainer, TrainingArguments\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msklearn\u001b[39;00m\n\u001b[0;32m      4\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[1;32m----> 5\u001b[0m     output_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./experiments/\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m run_name,\n\u001b[0;32m      6\u001b[0m     learning_rate\u001b[39m=\u001b[39m\u001b[39m1e-5\u001b[39m, \u001b[39m# lower learning rate.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     per_device_train_batch_size\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m,\n\u001b[0;32m      8\u001b[0m     per_device_eval_batch_size\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m,\n\u001b[0;32m      9\u001b[0m     num_train_epochs\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m,\n\u001b[0;32m     10\u001b[0m     weight_decay\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m,\n\u001b[0;32m     11\u001b[0m     do_eval\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m# We just want to train the model. Not eval objective.\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     save_strategy\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     13\u001b[0m     \u001b[39m# load_best_model_at_end=True,\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     report_to\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mwandb\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     15\u001b[0m     run_name\u001b[39m=\u001b[39mrun_name\n\u001b[0;32m     16\u001b[0m )\n\u001b[0;32m     18\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[0;32m     19\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m     20\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[0;32m     21\u001b[0m     train_dataset\u001b[39m=\u001b[39mdata[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m     22\u001b[0m     tokenizer\u001b[39m=\u001b[39mtokenizer,\n\u001b[0;32m     23\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'run_name' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import sklearn\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./experiments/\" + run_name,\n",
    "    learning_rate=1e-5, # lower learning rate.\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=8,\n",
    "    weight_decay=0.01,\n",
    "    do_eval=False, # We just want to train the model. Not eval objective.\n",
    "    save_strategy=\"epoch\",\n",
    "    # load_best_model_at_end=True,\n",
    "    report_to='wandb',\n",
    "    run_name=run_name\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=data['train'],\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "YWHOLli0GdWi",
    "outputId": "d04e8311-3bc4-417d-8600-f55d854c4a84"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: question_1, question_2, dr_id. If question_1, question_2, dr_id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2834\n",
      "  Num Epochs = 8\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2840\n",
      "  Number of trainable parameters = 108311810\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2840' max='2840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2840/2840 35:20, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.493627</td>\n",
       "      <td>0.790000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.502200</td>\n",
       "      <td>0.520202</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.267700</td>\n",
       "      <td>0.768769</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.267700</td>\n",
       "      <td>0.972102</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.118500</td>\n",
       "      <td>1.013869</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.032900</td>\n",
       "      <td>1.261037</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.032900</td>\n",
       "      <td>1.179059</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.012000</td>\n",
       "      <td>1.199661</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: question_1, question_2, dr_id. If question_1, question_2, dr_id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 214\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./experiments/behavioural_training/checkpoint-355\n",
      "Configuration saved in ./experiments/behavioural_training/checkpoint-355/config.json\n",
      "Model weights saved in ./experiments/behavioural_training/checkpoint-355/pytorch_model.bin\n",
      "tokenizer config file saved in ./experiments/behavioural_training/checkpoint-355/tokenizer_config.json\n",
      "Special tokens file saved in ./experiments/behavioural_training/checkpoint-355/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: question_1, question_2, dr_id. If question_1, question_2, dr_id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 214\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./experiments/behavioural_training/checkpoint-710\n",
      "Configuration saved in ./experiments/behavioural_training/checkpoint-710/config.json\n",
      "Model weights saved in ./experiments/behavioural_training/checkpoint-710/pytorch_model.bin\n",
      "tokenizer config file saved in ./experiments/behavioural_training/checkpoint-710/tokenizer_config.json\n",
      "Special tokens file saved in ./experiments/behavioural_training/checkpoint-710/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: question_1, question_2, dr_id. If question_1, question_2, dr_id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 214\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./experiments/behavioural_training/checkpoint-1065\n",
      "Configuration saved in ./experiments/behavioural_training/checkpoint-1065/config.json\n",
      "Model weights saved in ./experiments/behavioural_training/checkpoint-1065/pytorch_model.bin\n",
      "tokenizer config file saved in ./experiments/behavioural_training/checkpoint-1065/tokenizer_config.json\n",
      "Special tokens file saved in ./experiments/behavioural_training/checkpoint-1065/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: question_1, question_2, dr_id. If question_1, question_2, dr_id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 214\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./experiments/behavioural_training/checkpoint-1420\n",
      "Configuration saved in ./experiments/behavioural_training/checkpoint-1420/config.json\n",
      "Model weights saved in ./experiments/behavioural_training/checkpoint-1420/pytorch_model.bin\n",
      "tokenizer config file saved in ./experiments/behavioural_training/checkpoint-1420/tokenizer_config.json\n",
      "Special tokens file saved in ./experiments/behavioural_training/checkpoint-1420/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: question_1, question_2, dr_id. If question_1, question_2, dr_id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 214\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./experiments/behavioural_training/checkpoint-1775\n",
      "Configuration saved in ./experiments/behavioural_training/checkpoint-1775/config.json\n",
      "Model weights saved in ./experiments/behavioural_training/checkpoint-1775/pytorch_model.bin\n",
      "tokenizer config file saved in ./experiments/behavioural_training/checkpoint-1775/tokenizer_config.json\n",
      "Special tokens file saved in ./experiments/behavioural_training/checkpoint-1775/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: question_1, question_2, dr_id. If question_1, question_2, dr_id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 214\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./experiments/behavioural_training/checkpoint-2130\n",
      "Configuration saved in ./experiments/behavioural_training/checkpoint-2130/config.json\n",
      "Model weights saved in ./experiments/behavioural_training/checkpoint-2130/pytorch_model.bin\n",
      "tokenizer config file saved in ./experiments/behavioural_training/checkpoint-2130/tokenizer_config.json\n",
      "Special tokens file saved in ./experiments/behavioural_training/checkpoint-2130/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: question_1, question_2, dr_id. If question_1, question_2, dr_id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 214\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./experiments/behavioural_training/checkpoint-2485\n",
      "Configuration saved in ./experiments/behavioural_training/checkpoint-2485/config.json\n",
      "Model weights saved in ./experiments/behavioural_training/checkpoint-2485/pytorch_model.bin\n",
      "tokenizer config file saved in ./experiments/behavioural_training/checkpoint-2485/tokenizer_config.json\n",
      "Special tokens file saved in ./experiments/behavioural_training/checkpoint-2485/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: question_1, question_2, dr_id. If question_1, question_2, dr_id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 214\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./experiments/behavioural_training/checkpoint-2840\n",
      "Configuration saved in ./experiments/behavioural_training/checkpoint-2840/config.json\n",
      "Model weights saved in ./experiments/behavioural_training/checkpoint-2840/pytorch_model.bin\n",
      "tokenizer config file saved in ./experiments/behavioural_training/checkpoint-2840/tokenizer_config.json\n",
      "Special tokens file saved in ./experiments/behavioural_training/checkpoint-2840/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./experiments/behavioural_training/checkpoint-355 (score: 0.49362653493881226).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2840, training_loss=0.16475957799965227, metrics={'train_runtime': 2124.0411, 'train_samples_per_second': 10.674, 'train_steps_per_second': 1.337, 'total_flos': 5965253847121920.0, 'train_loss': 0.16475957799965227, 'epoch': 8.0})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Store Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-QaJ4zaM600l",
    "outputId": "e890f719-58fa-4c22-e77f-f3e93331bee5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./experiments/behavioural_training/checkpoint-2485)... Done. 6.7s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<wandb.sdk.wandb_artifacts.Artifact at 0x7fc602499d90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Log model\n",
    "\n",
    "artifact = wandb.Artifact('classifier', type='model')\n",
    "artifact.add_dir('./experiments/behavioural_training/checkpoint-2485')\n",
    "wandb.log_artifact(artifact)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 24 2022, 14:07:00) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "99001e2704ac6f7d0e01cf3f3c65c4a3c3569f1fa86956f5c9ed2a5b3d759ab2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
